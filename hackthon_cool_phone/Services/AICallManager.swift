// MARK: - AI é€šè¯ç®¡ç†å™¨ï¼ˆä¿®å¤éŸ³é¢‘å†²çªç‰ˆï¼‰

import Foundation
import SwiftUI
import SwiftData
import AVFoundation
import Speech
import Combine

@MainActor
class AICallManager: ObservableObject {
    static let shared = AICallManager()
    
    @Published var isAIEnabled = false
    @Published var conversationMessages: [ConversationMessage] = []
    @Published var isProcessing = false
    @Published var errorMessage: String?
    @Published var isRecording = false
    @Published var currentAudioLevel: Float = 0.0
    @Published var recognizedText: String = ""
    
    private let miniMaxService = MiniMaxVoiceService.shared
    
    private var audioLevelTimer: Timer?
    private var silenceTimer: Timer?
    
    // è¯­éŸ³è¯†åˆ«ï¼ˆç»Ÿä¸€ä½¿ç”¨ AVAudioEngineï¼‰
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // â­ éŸ³é‡ç›‘æµ‹å˜é‡
    private var currentPower: Float = -160.0  // å½“å‰éŸ³é‡ï¼ˆåˆ†è´ï¼‰
    
    // é™éŸ³æ£€æµ‹é…ç½®
    private let silenceThreshold: Float = -30.0      // è°ƒæ•´é˜ˆå€¼
    private let silenceDuration: TimeInterval = 1.5  // å¢åŠ åˆ° 1.5 ç§’
    private var lastSoundTime: Date = Date()
    private var hasSpokeOnce: Bool = false
    private var continuousSilenceCount: Int = 0
    
    private init() {
        print("âš™ï¸ AICallManager åˆå§‹åŒ–")
    }
    
    // MARK: - åˆå§‹åŒ– AI é€šè¯
    func initializeAICallWithAutoRecording(scenario: CallScenario, customText: String = "") async {
        print("\nğŸ¬ ===== åˆå§‹åŒ– AI é€šè¯ =====")
        print("ğŸ“‹ åœºæ™¯: \(scenario.rawValue)")
        
        let micAuthorized = await requestMicrophonePermission()
        let speechAuthorized = await requestSpeechRecognitionPermission()
        
        guard micAuthorized && speechAuthorized else {
            errorMessage = "éœ€è¦éº¦å…‹é£å’Œè¯­éŸ³è¯†åˆ«æƒé™"
            print("âŒ æƒé™ä¸è¶³")
            return
        }
        
        print("âœ… æƒé™å·²è·å–")
        
        miniMaxService.setupScenario(scenario, customText: customText)
        
        conversationMessages = []
        isAIEnabled = true
        
        print("ğŸ¯ å¼€å§‹ç”Ÿæˆ AI å¼€åœºç™½...")
        await generateAIGreeting()
        
        print("ğŸ™ï¸ å‡†å¤‡å¼€å§‹è‡ªåŠ¨å½•éŸ³...")
        await startAutoRecording()
        
        print("===========================\n")
    }
    
    // MARK: - è¯·æ±‚æƒé™
    private func requestMicrophonePermission() async -> Bool {
        print("ğŸ¤ è¯·æ±‚éº¦å…‹é£æƒé™...")
        return await withCheckedContinuation { continuation in
            AVAudioSession.sharedInstance().requestRecordPermission { granted in
                print(granted ? "âœ… éº¦å…‹é£æƒé™å·²æˆäºˆ" : "âŒ éº¦å…‹é£æƒé™è¢«æ‹’ç»")
                continuation.resume(returning: granted)
            }
        }
    }
    
    private func requestSpeechRecognitionPermission() async -> Bool {
        print("ğŸ—£ï¸ è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™...")
        return await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                let granted = status == .authorized
                print(granted ? "âœ… è¯­éŸ³è¯†åˆ«æƒé™å·²æˆäºˆ" : "âŒ è¯­éŸ³è¯†åˆ«æƒé™è¢«æ‹’ç»")
                continuation.resume(returning: granted)
            }
        }
    }
    
    // MARK: - AI å¼€åœºç™½
    private func generateAIGreeting() async {
        print("\nğŸ‘‹ ===== ç”Ÿæˆ AI å¼€åœºç™½ =====")
        isProcessing = true
        
        do {
            let greetingPrompt = "è¯·ç”¨ç®€çŸ­çš„ä¸€å¥è¯æ‰“æ‹›å‘¼å¹¶è¯´æ˜æ¥æ„ï¼Œä¸è¶…è¿‡20ä¸ªå­—"
            print("ğŸ“¤ å‘é€æç¤ºè¯: \(greetingPrompt)")
            
            let (audioData, textResponse) = try await miniMaxService.sendTextMessage(greetingPrompt)
            
            print("ğŸ“¥ AI å›å¤: \(textResponse)")
            print("ğŸ“Š éŸ³é¢‘å¤§å°: \(audioData.count) bytes")
            
            let message = ConversationMessage(text: textResponse, isUser: false)
            conversationMessages.append(message)
            
            print("ğŸ”§ é…ç½®éŸ³é¢‘ä¼šè¯ä¸ºæ’­æ”¾æ¨¡å¼...")
            try await configureAudioSessionForPlayback()
            
            print("ğŸ”Š æ’­æ”¾å¼€åœºç™½éŸ³é¢‘...")
            try await miniMaxService.playAudio(audioData)
            print("âœ… å¼€åœºç™½æ’­æ”¾å®Œæˆ")
            
        } catch {
            errorMessage = "AI åˆå§‹åŒ–å¤±è´¥: \(error.localizedDescription)"
            print("âŒ AI å¼€åœºç™½å¤±è´¥: \(error)")
        }
        
        isProcessing = false
        print("===========================\n")
    }
    
    // MARK: - é…ç½®éŸ³é¢‘ä¼šè¯
    private func configureAudioSessionForPlayback() async throws {
        let audioSession = AVAudioSession.sharedInstance()
        
        print("ğŸ”§ é…ç½®éŸ³é¢‘ä¼šè¯ [æ’­æ”¾æ¨¡å¼]")
        
        try audioSession.setCategory(.playback, mode: .default, options: [.duckOthers])
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        
        print("âœ… æ’­æ”¾æ¨¡å¼å·²æ¿€æ´»")
    }
    
    private func configureAudioSessionForRecording() async throws {
        let audioSession = AVAudioSession.sharedInstance()
        
        print("ğŸ”§ é…ç½®éŸ³é¢‘ä¼šè¯ [å½•éŸ³æ¨¡å¼]")
        
        try audioSession.setCategory(.record, mode: .measurement, options: [])
        try audioSession.setActive(true)
        
        print("âœ… å½•éŸ³æ¨¡å¼å·²æ¿€æ´»")
        print("ğŸ“± éŸ³é¢‘ç±»åˆ«: \(audioSession.category)")
    }
    
    // MARK: - å¼€å§‹è‡ªåŠ¨å½•éŸ³ï¼ˆä¿®å¤å¹¶å‘é”™è¯¯ç‰ˆï¼‰
    nonisolated private func startAutoRecording() async {
        print("\nğŸ¤ ===== å¼€å§‹è‡ªåŠ¨å½•éŸ³ =====")
        
        do {
            try await configureAudioSessionForRecording()
            
            // ç­‰å¾…é…ç½®ç”Ÿæ•ˆ
            try await Task.sleep(nanoseconds: 200_000_000) // 0.2ç§’
            
            await MainActor.run {
                self.isRecording = true
                self.lastSoundTime = Date()
                self.hasSpokeOnce = false
                self.continuousSilenceCount = 0
                self.currentPower = -160.0
            }
            
            await MainActor.run {
                self.startSpeechRecognitionWithVolumeMonitoring()
                self.startSilenceDetection()
            }
            
            print("âœ… å½•éŸ³å’Œè¯†åˆ«å·²å¯åŠ¨")
            print("ğŸ¯ ç­‰å¾…ç”¨æˆ·è¯´è¯...")
            print("===========================\n")
            
        } catch {
            await MainActor.run {
                self.errorMessage = "å¯åŠ¨å½•éŸ³å¤±è´¥: \(error.localizedDescription)"
            }
            print("âŒ å½•éŸ³å¯åŠ¨å¤±è´¥: \(error)")
        }
    }
    
    // MARK: - è¯­éŸ³è¯†åˆ« + éŸ³é‡ç›‘æµ‹ï¼ˆä¸å®æ—¶è¾“å‡ºè½¬å†™å†…å®¹ç‰ˆï¼‰
    private func startSpeechRecognitionWithVolumeMonitoring() {
        print("ğŸ—£ï¸ å¯åŠ¨è¯­éŸ³è¯†åˆ«å¼•æ“...")
        
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            print("âš ï¸ è¯­éŸ³è¯†åˆ«ä¸å¯ç”¨")
            return
        }
        
        // åœæ­¢ä¹‹å‰çš„ä»»åŠ¡
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // åœæ­¢ä¹‹å‰çš„éŸ³é¢‘å¼•æ“
        if audioEngine.isRunning {
            audioEngine.stop()
            if audioEngine.inputNode.numberOfInputs > 0 {
                audioEngine.inputNode.removeTap(onBus: 0)
            }
        }
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            print("âŒ åˆ›å»ºè¯†åˆ«è¯·æ±‚å¤±è´¥")
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        recognitionRequest.taskHint = .dictation
        
        let inputNode = audioEngine.inputNode
        
        // â­ æ˜¾å¼åˆ›å»ºæœ‰æ•ˆçš„éŸ³é¢‘æ ¼å¼
        let recordingFormat: AVAudioFormat
        
        if let nodeFormat = inputNode.inputFormat(forBus: 0) as AVAudioFormat?,
           nodeFormat.sampleRate > 0 && nodeFormat.channelCount > 0 {
            recordingFormat = nodeFormat
            print("âœ… ä½¿ç”¨è¾“å…¥èŠ‚ç‚¹æ ¼å¼")
        } else if let nodeFormat = inputNode.outputFormat(forBus: 0) as AVAudioFormat?,
                  nodeFormat.sampleRate > 0 && nodeFormat.channelCount > 0 {
            recordingFormat = nodeFormat
            print("âœ… ä½¿ç”¨è¾“å‡ºèŠ‚ç‚¹æ ¼å¼")
        } else if let standardFormat = AVAudioFormat(
            commonFormat: .pcmFormatFloat32,
            sampleRate: 16000.0,
            channels: 1,
            interleaved: false
        ) {
            recordingFormat = standardFormat
            print("âœ… ä½¿ç”¨æ ‡å‡†æ ¼å¼ï¼ˆ16kHz, å•å£°é“ï¼‰")
        } else {
            print("âŒ æ— æ³•åˆ›å»ºæœ‰æ•ˆçš„éŸ³é¢‘æ ¼å¼")
            return
        }
        
        print("ğŸ”§ éŸ³é¢‘æ ¼å¼:")
        print("   é‡‡æ ·ç‡: \(recordingFormat.sampleRate) Hz")
        print("   å£°é“æ•°: \(recordingFormat.channelCount)")
        print("   æ ¼å¼ID: \(recordingFormat.commonFormat.rawValue)")
        
        guard recordingFormat.sampleRate > 0 && recordingFormat.channelCount > 0 else {
            print("âŒ éŸ³é¢‘æ ¼å¼æ— æ•ˆ")
            return
        }
        
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self else { return }
            
            if let result = result {
                // ä¸å†å®æ—¶è¾“å‡º/æ›´æ–° recognizedTextï¼Œåªç”¨äºé™éŸ³æ£€æµ‹é€»è¾‘
                let interimText = result.bestTranscription.formattedString
                if !interimText.isEmpty {
                    Task { @MainActor in
                        if !self.hasSpokeOnce { self.hasSpokeOnce = true }
                        self.lastSoundTime = Date()
                    }
                }
                
                // â­ åªåœ¨æœ€ç»ˆç»“æœæ—¶æ›´æ–° recognizedTextï¼ˆé¿å…å®æ—¶è½¬å†™è¾“å‡ºï¼‰
                if result.isFinal {
                    let finalText = result.bestTranscription.formattedString
                    Task { @MainActor in
                        self.recognizedText = finalText
                    }
                }
            }
            
            if let error = error {
                let nsError = error as NSError
                if nsError.domain != "kLSRErrorDomain" || nsError.code != 203 {
                    print("âš ï¸ è¯†åˆ«é”™è¯¯: \(error.localizedDescription)")
                }
            }
        }
        
        // â­ å®‰å…¨åœ°å®‰è£… tap
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
            guard let self else { return }
            
            recognitionRequest.append(buffer)
            self.calculateAudioLevel(from: buffer)
        }
        
        print("âœ… éŸ³é¢‘ tap å·²å®‰è£…")
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
            print("âœ… è¯­éŸ³è¯†åˆ«å¼•æ“å·²å¯åŠ¨")
            print("ğŸ¤ å¼€å§‹ç›‘å¬...")
        } catch {
            print("âŒ è¯­éŸ³è¯†åˆ«å¼•æ“å¯åŠ¨å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    // MARK: - ä»éŸ³é¢‘ç¼“å†²åŒºè®¡ç®—éŸ³é‡
    private func calculateAudioLevel(from buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.floatChannelData else { return }
        
        let channelDataValue = channelData.pointee
        let channelDataValueArray = stride(from: 0, to: Int(buffer.frameLength), by: buffer.stride).map { channelDataValue[$0] }
        
        let rms = sqrt(channelDataValueArray.map { $0 * $0 }.reduce(0, +) / Float(buffer.frameLength))
        let avgPower = 20 * log10(rms)
        
        Task { @MainActor in
            self.currentPower = avgPower
            
            let normalizedLevel = self.normalizeAudioLevel(avgPower)
            self.currentAudioLevel = normalizedLevel
            
            if avgPower > self.silenceThreshold {
                self.lastSoundTime = Date()
                self.continuousSilenceCount = 0
            } else {
                self.continuousSilenceCount += 1
            }
        }
    }
    
    // MARK: - é™éŸ³æ£€æµ‹
    private func startSilenceDetection() {
        silenceTimer = Timer.scheduledTimer(withTimeInterval: 0.5, repeats: true) { [weak self] _ in
            guard let self = self else { return }
            
            Task { @MainActor in
                guard self.hasSpokeOnce else { return }
                
                let silenceDuration = Date().timeIntervalSince(self.lastSoundTime)
                
                if silenceDuration >= self.silenceDuration && self.continuousSilenceCount >= 3 {
                    print("ğŸ”‡ æ£€æµ‹åˆ°é™éŸ³ (\(String(format: "%.1f", silenceDuration))ç§’)")
                    await self.stopRecordingAndProcess()
                }
            }
        }
    }
    
    private func normalizeAudioLevel(_ decibels: Float) -> Float {
        let minDb: Float = -60.0
        let maxDb: Float = 0.0
        let clampedDb = max(minDb, min(maxDb, decibels))
        return (clampedDb - minDb) / (maxDb - minDb)
    }
    
    // MARK: - åœæ­¢å½•éŸ³å¹¶å¤„ç†
    func stopRecordingAndProcess() async {
        print("\nâ¹ï¸ ===== åœæ­¢å½•éŸ³å¹¶å¤„ç† =====")
        
        audioLevelTimer?.invalidate()
        silenceTimer?.invalidate()
        
        audioEngine.stop()
        if audioEngine.inputNode.numberOfInputs > 0 {
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // â­ ç»“æŸéŸ³é¢‘è¾“å…¥ï¼Œè®©è¯†åˆ«æ›´å¿«äº§å‡º final
        recognitionRequest?.endAudio()
        
        recognitionTask?.finish()
        recognitionTask = nil
        recognitionRequest = nil
        
        isRecording = false
        currentAudioLevel = 0.0
        
        guard !recognizedText.isEmpty else {
            print("âš ï¸ æœªè¯†åˆ«åˆ°æ–‡å­—ï¼Œé‡æ–°å¼€å§‹å½•éŸ³")
            recognizedText = ""
            hasSpokeOnce = false
            await startAutoRecording()
            return
        }
        
        let userText = recognizedText
        print("âœ… ç”¨æˆ·è¯´è¯å®Œæˆ: \(userText)")
        
        isProcessing = true
        
        let userMessage = ConversationMessage(text: userText, isUser: true)
        conversationMessages.append(userMessage)
        recognizedText = ""
        hasSpokeOnce = false
        
        do {
            print("\nğŸ“¤ å‘é€ç»™å¤§æ¨¡å‹...")
            let (audioData, textResponse) = try await miniMaxService.sendTextMessage(userText)
            
            print("ğŸ“¥ AI å›å¤: \(textResponse)")
            print("ğŸ“Š éŸ³é¢‘å¤§å°: \(audioData.count) bytes")
            
            let message = ConversationMessage(text: textResponse, isUser: false)
            conversationMessages.append(message)
            
            print("\nğŸ”§ åˆ‡æ¢åˆ°æ’­æ”¾æ¨¡å¼...")
            try await configureAudioSessionForPlayback()
            
            print("ğŸ”Š æ’­æ”¾ AI è¯­éŸ³å›å¤...")
            try await miniMaxService.playAudio(audioData)
            
            print("âœ… AI è¯­éŸ³æ’­æ”¾å®Œæˆ")
            print("âœ… ä¸€è½®å¯¹è¯å®Œæˆ")
            
        } catch {
            errorMessage = "å¤„ç†å¤±è´¥: \(error.localizedDescription)"
            print("âŒ å¤„ç†å¤±è´¥: \(error)")
        }
        
        isProcessing = false
        print("===========================\n")
        
        print("ğŸ”„ å‡†å¤‡ä¸‹ä¸€è½®å½•éŸ³...")
        await startAutoRecording()
    }
    
    // MARK: - ç»“æŸé€šè¯
    func endAICall() {
        print("\nğŸ“ ===== ç»“æŸé€šè¯ =====")
        
        audioLevelTimer?.invalidate()
        silenceTimer?.invalidate()
        
        audioEngine.stop()
        if audioEngine.inputNode.numberOfInputs > 0 {
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil
        
        miniMaxService.stopAudio()
        miniMaxService.resetConversation()
        
        isAIEnabled = false
        conversationMessages = []
        isRecording = false
        currentAudioLevel = 0.0
        recognizedText = ""
        hasSpokeOnce = false
        continuousSilenceCount = 0
        
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
            print("âœ… éŸ³é¢‘ä¼šè¯å·²æ¸…ç†")
        } catch {
            print("âš ï¸ éŸ³é¢‘ä¼šè¯æ¸…ç†å¤±è´¥: \(error)")
        }
        
        print("âœ… é€šè¯å·²ç»“æŸ")
        print("===========================\n")
    }
}
